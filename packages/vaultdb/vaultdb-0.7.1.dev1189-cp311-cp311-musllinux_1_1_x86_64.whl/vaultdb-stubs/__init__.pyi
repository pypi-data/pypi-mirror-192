# to regenerate this from scratch, run scripts/regenerate_python_stubs.sh .
# be warned - currently there are still tweaks needed after this file is
# generated. These should be annotated with a comment like
# # stubgen override
# to help the sanity of maintainers.
# stubgen override - missing import of Set
from typing import Any, ClassVar, Set, Optional

from typing import overload, Dict, List
import pandas
# stubgen override - unfortunately we need this for version checks
import sys
import fsspec
import pyarrow.lib
# stubgen override - This should probably not be exposed
#_clean_default_connection: Any
apilevel: str
comment: token_type
default_connection: VaultDBPyConnection
identifier: token_type
keyword: token_type
numeric_const: token_type
operator: token_type
paramstyle: str
string_const: token_type
threadsafety: int
__standard_vector_size__: int

__interactive__: bool
__jupyter__: bool

class BinderException(ProgrammingError): ...

class CastException(DataError): ...

class CatalogException(ProgrammingError): ...

class ConnectionException(OperationalError): ...

class ConstraintException(IntegrityError): ...

class ConversionException(DataError): ...

class DataError(Error): ...

class VaultDBPyConnection:
    def __init__(self, *args, **kwargs) -> None: ...
    def append(self, table_name: str, df: pandas.DataFrame) -> VaultDBPyConnection: ...
    def arrow(self, chunk_size: int = ...) -> pyarrow.lib.Table: ...
    def begin(self) -> VaultDBPyConnection: ...
    def close(self) -> None: ...
    def commit(self) -> VaultDBPyConnection: ...
    def cursor(self) -> VaultDBPyConnection: ...
    def df(self) -> pandas.DataFrame: ...
    def duplicate(self) -> VaultDBPyConnection: ...
    def execute(self, query: str, parameters: object = ..., multiple_parameter_sets: bool = ...) -> VaultDBPyConnection: ...
    def executemany(self, query: str, parameters: object = ...) -> VaultDBPyConnection: ...
    def fetch_arrow_table(self, chunk_size: int = ...) -> pyarrow.lib.Table: ...
    def fetch_df(self, *args, **kwargs) -> pandas.DataFrame: ...
    def fetch_df_chunk(self, *args, **kwargs) -> pandas.DataFrame: ...
    def fetch_record_batch(self, chunk_size: int = ...) -> pyarrow.lib.RecordBatchReader: ...
    def fetchall(self) -> list: ...
    def fetchdf(self, *args, **kwargs) -> pandas.DataFrame: ...
    def fetchmany(self, size: int = ...) -> list: ...
    def fetchnumpy(self) -> dict: ...
    def fetchone(self) -> object: ...
    def from_arrow(self, arrow_object: object) -> VaultDBPyRelation: ...
    def read_csv(
        self,
        name: str,
        header: Optional[bool | int] = None,
        compression: Optional[str] = None,
        sep: Optional[str] = None,
        delimiter: Optional[str] = None,
        dtype: Optional[Dict[str, str] | List[str]] = None,
        na_values: Optional[str] = None,
        skiprows: Optional[int] = None,
        quotechar: Optional[str] = None,
        escapechar: Optional[str] = None,
        encoding: Optional[str] = None,
        parallel: Optional[bool] = None,
        date_format: Optional[str] = None,
        timestamp_format: Optional[str] = None,
        sample_size: Optional[int] = None,
        all_varchar: Optional[bool] = None,
        normalize_names: Optional[bool] = None,
        filename: Optional[bool] = None,
    ) -> VaultDBPyRelation: ...
    def from_csv_auto(
        self,
        name: str,
        header: Optional[bool | int] = None,
        compression: Optional[str] = None,
        sep: Optional[str] = None,
        delimiter: Optional[str] = None,
        dtype: Optional[Dict[str, str] | List[str]] = None,
        na_values: Optional[str] = None,
        skiprows: Optional[int] = None,
        quotechar: Optional[str] = None,
        escapechar: Optional[str] = None,
        encoding: Optional[str] = None,
        parallel: Optional[bool] = None,
        date_format: Optional[str] = None,
        timestamp_format: Optional[str] = None,
        sample_size: Optional[int] = None,
        all_varchar: Optional[bool] = None,
        normalize_names: Optional[bool] = None,
        filename: Optional[bool] = None,
    ) -> VaultDBPyRelation: ...
    def from_df(self, df: pandas.DataFrame = ...) -> VaultDBPyRelation: ...
    @overload
    def read_parquet(self, file_glob: str, binary_as_string: bool = ..., *, file_row_number: bool = ..., filename: bool = ..., hive_partitioning: bool = ..., union_by_name: bool = ...) -> VaultDBPyRelation: ...
    @overload
    def read_parquet(self, file_globs: List[str], binary_as_string: bool = ..., *, file_row_number: bool = ..., filename: bool = ..., hive_partitioning: bool = ..., union_by_name: bool = ...) -> VaultDBPyRelation: ...
    @overload
    def from_parquet(self, file_glob: str, binary_as_string: bool = ..., *, file_row_number: bool = ..., filename: bool = ..., hive_partitioning: bool = ..., union_by_name: bool = ...) -> VaultDBPyRelation: ...
    @overload
    def from_parquet(self, file_globs: List[str], binary_as_string: bool = ..., *, file_row_number: bool = ..., filename: bool = ..., hive_partitioning: bool = ..., union_by_name: bool = ...) -> VaultDBPyRelation: ...
    def from_query(self, query: str, alias: str = ...) -> VaultDBPyRelation: ...
    def from_substrait(self, proto: bytes) -> VaultDBPyRelation: ...
    def get_substrait(self, query: str) -> VaultDBPyRelation: ...
    def get_substrait_json(self, query: str) -> VaultDBPyRelation: ...
    def from_substrait_json(self, json: str) -> VaultDBPyRelation: ...
    def get_table_names(self, query: str) -> Set[str]: ...
    def install_extension(self, *args, **kwargs) -> None: ...
    def list_filesystems(self) -> list: ...
    def load_extension(self, extension: str) -> None: ...
    def query(self, query: str, alias: str = ...) -> VaultDBPyRelation: ...
    def register(self, view_name: str, python_object: object) -> VaultDBPyConnection: ...
    def register_filesystem(self, filesystem: fsspec.AbstractFileSystem) -> None: ...
    def rollback(self) -> VaultDBPyConnection: ...
    def table(self, table_name: str) -> VaultDBPyRelation: ...
    def table_function(self, name: str, parameters: object = ...) -> VaultDBPyRelation: ...
    def unregister(self, view_name: str) -> VaultDBPyConnection: ...
    def unregister_filesystem(self, name: str) -> None: ...
    def values(self, values: object) -> VaultDBPyRelation: ...
    def view(self, view_name: str) -> VaultDBPyRelation: ...
    def __enter__(self) -> VaultDBPyConnection: ...
    def __exit__(self, exc_type: object, exc: object, traceback: object) -> bool: ...
    @property
    def description(self) -> object: ...

class VaultDBPyRelation:
    def close(self) -> None: ...
    def __init__(self, *args, **kwargs) -> None: ...
    def abs(self, aggregation_columns: str) -> VaultDBPyRelation: ...
    def aggregate(self, aggr_expr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def apply(self, function_name: str, function_aggr: str, group_expr: str = ..., function_parameter: str = ..., projected_columns: str = ...) -> VaultDBPyRelation: ...
    def arrow(self, batch_size: int = ...) -> pyarrow.lib.Table: ...
    def count(self, count_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def create(self, table_name: str) -> None: ...
    def create_view(self, view_name: str, replace: bool = ...) -> VaultDBPyRelation: ...
    def cummax(self, aggregation_columns: str) -> VaultDBPyRelation: ...
    def cummin(self, aggregation_columns: str) -> VaultDBPyRelation: ...
    def cumprod(self, aggregation_columns: str) -> VaultDBPyRelation: ...
    def cumsum(self, aggregation_columns: str) -> VaultDBPyRelation: ...
    def describe(self) -> VaultDBPyRelation: ...
    def df(self, *args, **kwargs) -> pandas.DataFrame: ...
    def distinct(self) -> VaultDBPyRelation: ...
    def except_(self, other_rel: VaultDBPyRelation) -> VaultDBPyRelation: ...
    def execute(self, *args, **kwargs) -> VaultDBPyRelation: ...
    def explain(self) -> str: ...
    def fetchall(self) -> object: ...
    def fetchmany(self, size: int = ...) -> list: ...
    def fetchnumpy(self) -> dict: ...
    def fetchone(self) -> object: ...
    def fetchdf(self, *args, **kwargs) -> Any: ...
    def fetch_arrow_table(self, chunk_size: int = ...) -> pyarrow.lib.Table: ...
    def filter(self, filter_expr: str) -> VaultDBPyRelation: ...
    def insert(self, values: object) -> None: ...
    def insert_into(self, table_name: str) -> None: ...
    def intersect(self, other_rel: VaultDBPyRelation) -> VaultDBPyRelation: ...
    def join(self, other_rel: VaultDBPyRelation, condition: str, how: str = ...) -> VaultDBPyRelation: ...
    def kurt(self, aggregation_columns: str, group_columns: str = ...) -> VaultDBPyRelation: ...
    def limit(self, n: int, offset: int = ...) -> VaultDBPyRelation: ...
    def mad(self, aggregation_columns: str, group_columns: str = ...) -> VaultDBPyRelation: ...
    def map(self, map_function: function) -> VaultDBPyRelation: ...
    def max(self, max_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def mean(self, mean_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def median(self, median_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def min(self, min_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def mode(self, aggregation_columns: str, group_columns: str = ...) -> VaultDBPyRelation: ...
    def order(self, order_expr: str) -> VaultDBPyRelation: ...
    def prod(self, aggregation_columns: str, group_columns: str = ...) -> VaultDBPyRelation: ...
    def project(self, project_expr: str) -> VaultDBPyRelation: ...
    def quantile(self, q: str, quantile_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def query(self, virtual_table_name: str, sql_query: str) -> VaultDBPyRelation: ...
    def record_batch(self, batch_size: int = ...) -> pyarrow.lib.RecordBatchReader: ...
    def fetch_arrow_reader(self, batch_size: int = ...) -> pyarrow.lib.RecordBatchReader: ...
    def sem(self, aggregation_columns: str, group_columns: str = ...) -> VaultDBPyRelation: ...
    def set_alias(self, alias: str) -> VaultDBPyRelation: ...
    def skew(self, aggregation_columns: str, group_columns: str = ...) -> VaultDBPyRelation: ...
    def std(self, std_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def sum(self, sum_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def to_arrow_table(self, batch_size: int = ...) -> pyarrow.lib.Table: ...
    def to_df(self, *args, **kwargs) -> pandas.DataFrame: ...
    def union(self, union_rel: VaultDBPyRelation) -> VaultDBPyRelation: ...
    def unique(self, unique_aggr: str) -> VaultDBPyRelation: ...
    def value_counts(self, value_counts_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def var(self, var_aggr: str, group_expr: str = ...) -> VaultDBPyRelation: ...
    def to_parquet(
        self,
        file_name: str,
        compression: Optional[str]
    ) -> None: ...
    def write_parquet(
        self,
        file_name: str,
        compression: Optional[str]
    ) -> None: ...
    def to_csv(
        self,
        file_name: str,
        sep: Optional[str],
        na_rep: Optional[str],
        header: Optional[bool],
        quotechar: Optional[str],
        escapechar: Optional[str],
        date_format: Optional[str],
        timestamp_format: Optional[str],
        quoting: Optional[str | int],
        encoding: Optional[str],
        compression: Optional[str]
    ) -> None: ...
    def write_csv(
        self,
        file_name: str,
        sep: Optional[str],
        na_rep: Optional[str],
        header: Optional[bool],
        quotechar: Optional[str],
        escapechar: Optional[str],
        date_format: Optional[str],
        timestamp_format: Optional[str],
        quoting: Optional[str | int],
        encoding: Optional[str],
        compression: Optional[str]
    ) -> None: ...
    def __len__(self) -> int: ...
    @property
    def alias(self) -> str: ...
    @property
    def columns(self) -> list: ...
    @property
    def dtypes(self) -> list: ...
    @property
    def description(self) -> object: ...
    @property
    def shape(self) -> tuple: ...
    @property
    def type(self) -> str: ...
    @property
    def types(self) -> list: ...

class Error(Exception): ...

class FatalException(Error): ...

class IOException(OperationalError): ...

class IntegrityError(Error): ...

class InternalError(Error): ...

class InternalException(InternalError): ...

class InterruptException(Error): ...

class InvalidInputException(ProgrammingError): ...

class InvalidTypeException(ProgrammingError): ...

class NotImplementedException(NotSupportedError): ...

class NotSupportedError(Error): ...

class OperationalError(Error): ...

class OutOfMemoryException(OperationalError): ...

class OutOfRangeException(DataError): ...

class ParserException(ProgrammingError): ...

class PermissionException(Error): ...

class ProgrammingError(Error): ...

class SequenceException(Error): ...

class SerializationException(OperationalError): ...

class StandardException(Error): ...

class SyntaxException(ProgrammingError): ...

class TransactionException(OperationalError): ...

class TypeMismatchException(DataError): ...

class ValueOutOfRangeException(DataError): ...

class Warning(Exception): ...

class token_type:
    # stubgen override - these make mypy sad
    #__doc__: ClassVar[str] = ...  # read-only
    #__members__: ClassVar[dict] = ...  # read-only
    __entries: ClassVar[dict] = ...
    comment: ClassVar[token_type] = ...
    identifier: ClassVar[token_type] = ...
    keyword: ClassVar[token_type] = ...
    numeric_const: ClassVar[token_type] = ...
    operator: ClassVar[token_type] = ...
    string_const: ClassVar[token_type] = ...
    def __init__(self, value: int) -> None: ...
    def __eq__(self, other: object) -> bool: ...
    def __getstate__(self) -> int: ...
    def __hash__(self) -> int: ...
    # stubgen override - pybind only puts index in python >= 3.8: https://github.com/EricCousineau-TRI/pybind11/blob/54430436/include/pybind11/pybind11.h#L1789
    if sys.version_info >= (3, 7):
        def __index__(self) -> int: ...
    def __int__(self) -> int: ...
    def __ne__(self, other: object) -> bool: ...
    def __setstate__(self, state: int) -> None: ...
    @property
    def name(self) -> str: ...
    @property
    def value(self) -> int: ...
    @property
    # stubgen override - this gets removed by stubgen but it shouldn't
    def __members__(self) -> object: ...

def aggregate(df: pandas.DataFrame, aggr_expr: str, group_expr: str = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def alias(df: pandas.DataFrame, alias: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def connect(database: str = ..., read_only: bool = ..., config: dict = ...) -> VaultDBPyConnection: ...
def distinct(df: pandas.DataFrame, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def filter(df: pandas.DataFrame, filter_expr: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def from_substrait_json(jsonm: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def limit(df: pandas.DataFrame, n: int, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def order(df: pandas.DataFrame, order_expr: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def project(df: pandas.DataFrame, project_expr: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def write_csv(df: pandas.DataFrame, file_name: str, connection: VaultDBPyConnection = ...) -> None: ...
def read_csv(
    name: str,
    header: Optional[bool | int] = None,
    compression: Optional[str] = None,
    sep: Optional[str] = None,
    delimiter: Optional[str] = None,
    dtype: Optional[Dict[str, str] | List[str]] = None,
    na_values: Optional[str] = None,
    skiprows: Optional[int] = None,
    quotechar: Optional[str] = None,
    escapechar: Optional[str] = None,
    encoding: Optional[str] = None,
    parallel: Optional[bool] = None,
    date_format: Optional[str] = None,
    timestamp_format: Optional[str] = None,
    sample_size: Optional[int] = None,
    all_varchar: Optional[bool] = None,
    normalize_names: Optional[bool] = None,
    filename: Optional[bool] = None,
    connection: VaultDBPyConnection = ...
) -> VaultDBPyRelation: ...
def from_csv_auto(
    name: str,
    header: Optional[bool | int] = None,
    compression: Optional[str] = None,
    sep: Optional[str] = None,
    delimiter: Optional[str] = None,
    dtype: Optional[Dict[str, str] | List[str]] = None,
    na_values: Optional[str] = None,
    skiprows: Optional[int] = None,
    quotechar: Optional[str] = None,
    escapechar: Optional[str] = None,
    encoding: Optional[str] = None,
    parallel: Optional[bool] = None,
    date_format: Optional[str] = None,
    timestamp_format: Optional[str] = None,
    sample_size: Optional[int] = None,
    all_varchar: Optional[bool] = None,
    normalize_names: Optional[bool] = None,
    filename: Optional[bool] = None,
    connection: VaultDBPyConnection = ...
) -> VaultDBPyRelation: ...

def append(table_name: str, df: pandas.DataFrame, connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def arrow(chunk_size: int = ..., connection: VaultDBPyConnection = ...) -> pyarrow.lib.Table: ...
def begin(connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def close(connection: VaultDBPyConnection = ...) -> None: ...
def commit(connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def cursor(connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def df(connection: VaultDBPyConnection = ...) -> pandas.DataFrame: ...
def duplicate(connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def execute(query: str, parameters: object = ..., multiple_parameter_sets: bool = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def executemany(query: str, parameters: object = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def fetch_arrow_table(chunk_size: int = ..., connection: VaultDBPyConnection = ...) -> pyarrow.lib.Table: ...
def fetch_df(*args, connection: VaultDBPyConnection = ..., **kwargs) -> pandas.DataFrame: ...
def fetch_df_chunk(*args, connection: VaultDBPyConnection = ..., **kwargs) -> pandas.DataFrame: ...
def fetch_record_batch(chunk_size: int = ..., connection: VaultDBPyConnection = ...) -> pyarrow.lib.RecordBatchReader: ...
def fetchall(connection: VaultDBPyConnection = ...) -> list: ...
def fetchdf(*args, connection: VaultDBPyConnection = ..., **kwargs) -> pandas.DataFrame: ...
def fetchmany(size: int = ..., connection: VaultDBPyConnection = ...) -> list: ...
def fetchnumpy(connection: VaultDBPyConnection = ...) -> dict: ...
def fetchone(connection: VaultDBPyConnection = ...) -> object: ...
def from_arrow(arrow_object: object, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def from_df(df: pandas.DataFrame = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
@overload
def read_parquet(file_glob: str, binary_as_string: bool = ..., *, file_row_number: bool = ..., filename: bool = ..., hive_partitioning: bool = ..., union_by_name: bool = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
@overload
def read_parquet(file_globs: List[str], binary_as_string: bool = ..., *, file_row_number: bool = ..., filename: bool = ..., hive_partitioning: bool = ..., union_by_name: bool = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
@overload
def from_parquet(file_glob: str, binary_as_string: bool = ..., *, file_row_number: bool = ..., filename: bool = ..., hive_partitioning: bool = ..., union_by_name: bool = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
@overload
def from_parquet(file_globs: List[str], binary_as_string: bool = ..., *, file_row_number: bool = ..., filename: bool = ..., hive_partitioning: bool = ..., union_by_name: bool = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def from_query(query: str, alias: str = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def from_substrait(proto: bytes, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def get_substrait(query: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def get_substrait_json(query: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def get_table_names(query: str, connection: VaultDBPyConnection = ...) -> Set[str]: ...
def install_extension(*args, connection: VaultDBPyConnection = ..., **kwargs) -> None: ...
def list_filesystems(connection: VaultDBPyConnection = ...) -> list: ...
def load_extension(extension: str, connection: VaultDBPyConnection = ...) -> None: ...
def query(query: str, alias: str = 'query_relation', connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def register(view_name: str, python_object: object, connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def register_filesystem(filesystem: fsspec.AbstractFileSystem, connection: VaultDBPyConnection = ...) -> None: ...
def rollback(connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def table(table_name: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def table_function(name: str, parameters: object = ..., connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def unregister(view_name: str, connection: VaultDBPyConnection = ...) -> VaultDBPyConnection: ...
def query_df(df: pandas.DataFrame, virtual_table_name: str, sql_query: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def unregister_filesystem(name: str, connection: VaultDBPyConnection = ...) -> None: ...
def tokenize(query: str) -> object: ...
def values(values: object, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def view(view_name: str, connection: VaultDBPyConnection = ...) -> VaultDBPyRelation: ...
def description(connection: VaultDBPyConnection = ...) -> object: ...

import datetime
import enum
from inspect import signature
from typing import Callable
from typing import Dict
from typing import List
from typing import Optional
from typing import Sequence
from typing import Union

import attr
import pendulum
from typeguard import typechecked

from tecton._internals.type_utils import to_spark_schema_wrapper
from tecton.aggregation_functions import AggregationFunction
from tecton.declarative.base import BaseDataSource
from tecton.declarative.base import BaseEntity
from tecton.declarative.base import BaseFeatureDefinition
from tecton.declarative.base import FeatureReference
from tecton.declarative.base import OutputStream
from tecton.declarative.data_source import RequestSource
from tecton.declarative.filtered_source import FilteredSource
from tecton.declarative.transformation import PipelineNodeWrapper
from tecton.features_common.feature_configs import DatabricksClusterConfig
from tecton.features_common.feature_configs import DEFAULT_SPARK_VERSIONS
from tecton.features_common.feature_configs import DeltaConfig
from tecton.features_common.feature_configs import DynamoConfig
from tecton.features_common.feature_configs import EMRClusterConfig
from tecton.features_common.feature_configs import MonitoringConfig
from tecton.features_common.feature_configs import ParquetConfig
from tecton.features_common.feature_configs import RedisConfig
from tecton_core import time_utils
from tecton_core.feature_definition_wrapper import FrameworkVersion
from tecton_core.feature_view_utils import construct_aggregation_output_feature_name
from tecton_core.feature_view_utils import resolve_function_name
from tecton_core.id_helper import IdHelper
from tecton_core.materialization_context import BaseMaterializationContext
from tecton_core.pipeline_common import transformation_type_checker
from tecton_proto.args import basic_info_pb2
from tecton_proto.args import feature_service_pb2
from tecton_proto.args import feature_view_pb2
from tecton_proto.args import pipeline_pb2
from tecton_proto.args.feature_view_pb2 import BatchTriggerType as BatchTriggerTypeProto
from tecton_proto.args.feature_view_pb2 import EntityKeyOverride
from tecton_proto.args.feature_view_pb2 import FeatureViewArgs
from tecton_proto.args.feature_view_pb2 import FeatureViewType
from tecton_proto.args.feature_view_pb2 import MaterializedFeatureViewArgs
from tecton_proto.args.feature_view_pb2 import StreamProcessingMode as StreamProcessingModeProto
from tecton_proto.common.data_source_type_pb2 import DataSourceType
from tecton_spark.spark_schema_wrapper import SparkSchemaWrapper

# This is the mode used when the feature view decorator is used on a pipeline function, i.e. one that only contains
# references to transformations and constants.
PIPELINE_MODE = "pipeline"

# This is used for the low latency streaming feature views.
CONTINUOUS_MODE = "continuous"

# FilteredSource start offsets smaller (more negative) than this offset will be considered UNBOUNDED_PRECEEDING.
MIN_START_OFFSET = datetime.timedelta(days=-365 * 100)  # 100 years

# Create a parallel enum class since Python proto extensions do not use an enum class.
# Keep up-to-date with StreamProcessingMode from tecton_proto/args/feature_view.proto.
class StreamProcessingMode(enum.Enum):
    TIME_INTERVAL = StreamProcessingModeProto.STREAM_PROCESSING_MODE_TIME_INTERVAL
    CONTINUOUS = StreamProcessingModeProto.STREAM_PROCESSING_MODE_CONTINUOUS


# Keep up-to-date with BatchTriggerType from tecton_proto/args/feature_view.proto.
class BatchTriggerType(enum.Enum):
    SCHEDULED = BatchTriggerTypeProto.BATCH_TRIGGER_TYPE_SCHEDULED
    MANUAL = BatchTriggerTypeProto.BATCH_TRIGGER_TYPE_MANUAL
    NO_BATCH_MATERIALIZATION = BatchTriggerTypeProto.BATCH_TRIGGER_TYPE_NO_BATCH_MATERIALIZATION


@attr.s(auto_attribs=True)
class Aggregation(object):
    """
    This class describes a single aggregation that is applied in a batch or stream feature view.

    :param column: Column name of the feature we are aggregating.
    :type column: str
    :param function: One of the built-in aggregation functions.
    :type function: Union[str, AggregationFunction]
    :param time_window: Duration to aggregate over. Example: ``datetime.timedelta(days=30)``.
    :type time_window: datetime.timedelta
    :param name: The name of this feature. Defaults to an autogenerated name, e.g. transaction_count_7d_1d.
    :type name: str

    `function` can be one of predefined numeric aggregation functions, namely ``"count"``, ``"sum"``, ``"mean"``, ``"min"``, ``"max"``, ``"var_samp"``, ``"var_pop"``, ``"variance"`` - alias for ``"var_samp"``, ``"stddev_samp"``, ``"stddev_pop"``, ``"stddev"`` - alias for ``"stddev_samp"``. For
    these numeric aggregations, you can pass the name of it as a string. Nulls are handled like Spark SQL `Function(column)`, e.g. SUM/MEAN/MIN/MAX/VAR_SAMP/VAR_POP/VAR/STDDEV_SAMP/STDDEV_POP/STDDEV of all nulls is null and COUNT of all nulls is 0.

    In addition to numeric aggregations, :class:`Aggregation` supports the last non-distinct and distinct N aggregation that will compute the last N non-distinct and distinct values for the column by timestamp. Right now only string column is supported as input to this aggregation, i.e., the resulting feature value will be a list of strings. The order of the value in the list is ascending based on the timestamp. Nulls are not included in the aggregated list.

    You can use it via the ```last()``` and  ``last_distinct()`` helper function like this:

    .. code-block:: python

        from tecton.aggregation_functions import last_distinct, last

        @batch_feature_view(
        ...
        aggregations=[
            Aggregation(
                column='my_column',
                function=last_distinct(15),
                time_window=datetime.timedelta(days=7)),
            Aggregation(
                column='my_column',
                function=last(15),
                time_window=datetime.timedelta(days=7)),
            ],
        ...
        )
        def my_fv(data_source):
            pass

    """

    column: str
    """Column name of the feature we are aggregating."""
    function: Union[str, AggregationFunction]
    """One of the built-in aggregation functions (`'count'`, `'sum'`, `'mean'`, `'min'`, `'max'`, `'var_samp'`, `'var_pop'`, `'variance'`, `'stddev_samp'`, `'stddev_pop'`, `'stddev'`)."""
    time_window: datetime.timedelta
    """Example: ``datetime.timedelta(days=30)``"""
    name: Optional[str] = None
    """Example: ``datetime.timedelta(days=30)``"""

    def _to_proto(self, aggregation_interval: datetime.timedelta, is_continuous: bool):
        proto = feature_view_pb2.FeatureAggregation()
        proto.column = self.column

        if isinstance(self.function, str):
            proto.function = self.function
        elif isinstance(self.function, AggregationFunction):
            proto.function = self.function.name
            for k, v in self.function.params.items():
                assert isinstance(v, int)
                proto.function_params[k].CopyFrom(feature_view_pb2.ParamValue(int64_value=v))
        else:
            raise TypeError(f"Invalid function type: {type(self.function)}")

        proto.time_window.FromTimedelta(self.time_window)

        if self.name:
            proto.name = self.name
        else:
            proto.name = construct_aggregation_output_feature_name(
                proto.column,
                resolve_function_name(proto.function, proto.function_params),
                proto.time_window,
                time_utils.timedelta_to_proto(aggregation_interval),
                is_continuous,
            )
        return proto


def get_source_input_params(user_function) -> List[str]:
    # Filter out the materailization context to avoid mapping data sources to it.
    return [
        param.name
        for param in signature(user_function).parameters.values()
        if not isinstance(param.default, BaseMaterializationContext)
    ]


def prepare_common_fv_args(basic_info, entities, pipeline_root) -> FeatureViewArgs:
    args = FeatureViewArgs()
    args.feature_view_id.CopyFrom(IdHelper.from_string(IdHelper.generate_string_id()))

    args.version = FrameworkVersion.FWV5.value

    args.info.CopyFrom(basic_info)

    args.entities.extend([EntityKeyOverride(entity_id=entity._id, join_keys=entity.join_keys) for entity in entities])
    args.pipeline.root.CopyFrom(pipeline_root)

    return args


def source_to_pipeline_node(
    source: Union[BaseDataSource, FilteredSource, RequestSource, BaseFeatureDefinition, FeatureReference],
    input_name: str,
) -> PipelineNodeWrapper:
    if isinstance(source, BaseFeatureDefinition):
        source = FeatureReference(feature_definition=source)

    pipeline_node = pipeline_pb2.PipelineNode()
    if isinstance(source, BaseDataSource):
        node = pipeline_pb2.DataSourceNode(
            virtual_data_source_id=source._id,
            window_unbounded=True,
            schedule_offset=time_utils.timedelta_to_proto(source.data_delay),
            input_name=input_name,
        )
        pipeline_node.data_source_node.CopyFrom(node)
    elif isinstance(source, FilteredSource):
        node = pipeline_pb2.DataSourceNode(
            virtual_data_source_id=source.source._id,
            schedule_offset=time_utils.timedelta_to_proto(source.source.data_delay),
            input_name=input_name,
        )
        if source.start_time_offset <= MIN_START_OFFSET:
            node.window_unbounded_preceding = True
        else:
            node.start_time_offset.FromTimedelta(source.start_time_offset)

        pipeline_node.data_source_node.CopyFrom(node)
    elif isinstance(source, RequestSource):
        request_schema = source.schema
        if isinstance(request_schema, List):
            wrapper = to_spark_schema_wrapper(request_schema)
        else:
            wrapper = SparkSchemaWrapper(request_schema)

        node = pipeline_pb2.RequestDataSourceNode(
            request_context=pipeline_pb2.RequestContext(schema=wrapper.to_proto()),
            input_name=input_name,
        )
        pipeline_node.request_data_source_node.CopyFrom(node)
    elif isinstance(source, FeatureReference):
        override_join_keys = None
        if source.override_join_keys:
            override_join_keys = [
                feature_service_pb2.ColumnPair(spine_column=spine_key, feature_column=fv_key)
                for fv_key, spine_key in sorted(source.override_join_keys.items())
            ]
        node = pipeline_pb2.FeatureViewNode(
            feature_view_id=source.id,
            input_name=input_name,
            feature_view=feature_service_pb2.FeatureServiceFeaturePackage(
                feature_package_id=source.id,
                override_join_keys=override_join_keys,
                namespace=source.namespace,
                features=source.features,
            ),
        )
        pipeline_node.feature_view_node.CopyFrom(node)
    else:
        raise TypeError(f"Invalid source type: {type(source)}")
    return PipelineNodeWrapper(node_proto=pipeline_node)


def transformation_to_pipeline_node(
    pipeline_function: Callable,
    params_to_sources: Dict[
        str, Union[BaseDataSource, FilteredSource, RequestSource, BaseFeatureDefinition, FeatureReference]
    ],
) -> PipelineNodeWrapper:
    pipeline_kwargs = sources_to_pipeline_nodes(params_to_sources)
    pipeline_fn_result = pipeline_function(**pipeline_kwargs)
    return pipeline_fn_result


def test_binding_user_function(fn, inputs):
    # this function binds the top-level pipeline function only, for transformation binding, see transformation.__call__
    pipeline_signature = signature(fn)
    try:
        pipeline_signature.bind(**inputs)
    except TypeError as e:
        raise TypeError(f"while binding inputs to pipeline function, TypeError: {e}")


def sources_to_pipeline_nodes(
    params_to_sources: Dict[
        str, Union[BaseDataSource, FilteredSource, RequestSource, BaseFeatureDefinition, FeatureReference]
    ]
) -> Dict[str, PipelineNodeWrapper]:
    kwargs = {}
    for param_name, source in params_to_sources.items():
        pipeline_node = source_to_pipeline_node(source=source, input_name=param_name)
        kwargs[param_name] = pipeline_node

    return kwargs


def _build_default_cluster_config():
    return feature_view_pb2.ClusterConfig(
        implicit_config=feature_view_pb2.DefaultClusterConfig(**DEFAULT_SPARK_VERSIONS)
    )


def build_pipeline_for_odfv(
    fv_name: str,
    user_function: Callable,
    pipeline_function: Callable[..., PipelineNodeWrapper],
    sources: Sequence[Union[RequestSource, FeatureReference, BaseFeatureDefinition]],
) -> PipelineNodeWrapper:
    fn_params = get_source_input_params(user_function)
    params_to_sources = dict(zip(fn_params, sources))
    pipeline_root = transformation_to_pipeline_node(pipeline_function, params_to_sources)
    # we bind to user_function since pipeline_function may be artificially created and just accept **kwargs
    test_binding_user_function(user_function, params_to_sources)

    transformation_type_checker(fv_name, pipeline_root.node_proto, "pipeline", ["pipeline", "pandas", "python"])
    return pipeline_root


@typechecked
def build_materialized_feature_view_args(
    name: str,
    pipeline: pipeline_pb2.Pipeline,
    entities: List[BaseEntity],
    online: bool,
    offline: bool,
    offline_store: Union[ParquetConfig, DeltaConfig],
    online_store: Optional[Union[DynamoConfig, RedisConfig]],
    aggregation_interval: Optional[datetime.timedelta],
    aggregations: Optional[List[Aggregation]],
    ttl: Optional[datetime.timedelta],
    feature_start_time: Optional[Union[pendulum.DateTime, datetime.datetime]],
    batch_schedule: Optional[datetime.timedelta],
    online_serving_index: Optional[List[str]],
    batch_compute: Optional[Union[DatabricksClusterConfig, EMRClusterConfig]],
    stream_compute: Optional[Union[DatabricksClusterConfig, EMRClusterConfig]],
    monitor_freshness: bool,
    expected_feature_freshness: Optional[datetime.timedelta],
    alert_email: Optional[str],
    description: Optional[str],
    owner: Optional[str],
    tags: Optional[Dict[str, str]],
    feature_view_type: FeatureViewType,
    timestamp_field: Optional[str],
    data_source_type: DataSourceType,
    incremental_backfills: bool,
    prevent_destroy: bool,
    stream_processing_mode: Optional[StreamProcessingMode] = None,
    max_batch_aggregation_interval: Optional[datetime.timedelta] = None,
    output_stream: Optional[OutputStream] = None,
    batch_trigger: Optional[BatchTriggerType] = None,
) -> FeatureViewArgs:
    """Build feature view args proto for materialized feature views (i.e. batch and stream feature views)."""
    batch_compute_proto = batch_compute._to_cluster_proto() if batch_compute else None

    stream_compute_proto = None
    if stream_compute:
        stream_compute_proto = stream_compute._to_cluster_proto()
    elif data_source_type == DataSourceType.STREAM_WITH_BATCH:
        stream_compute_proto = _build_default_cluster_config()

    monitoring = MonitoringConfig(monitor_freshness, expected_feature_freshness, alert_email)

    aggregation_protos = None
    if aggregations:
        # TODO(jake): Move these assertions to backend validations.
        assert ttl is None, "`ttl` is automatically set for aggregations to the `aggregation_interval`"
        assert not incremental_backfills, "`incremental_backfills` cannot be used with aggregations"

        is_continuous = stream_processing_mode == StreamProcessingMode.CONTINUOUS
        if aggregation_interval is None:
            aggregation_interval = datetime.timedelta(seconds=0)

        if is_continuous:
            assert aggregation_interval == datetime.timedelta(
                seconds=0
            ), "If `stream_processing_mode=StreamProcessingMode.CONTINUOUS`, `aggregation_interval` must be unset or zero."
        else:
            assert aggregation_interval > datetime.timedelta(
                seconds=0
            ), "`aggregation_interval` or `stream_processing_mode=StreamProcessingMode.CONTINUOUS` is required if specifying aggregations."

        aggregation_protos = [agg._to_proto(aggregation_interval, is_continuous) for agg in aggregations]
    else:
        assert aggregation_interval is None, "`aggregation_interval` can only be specified when using `aggregations`"

    return FeatureViewArgs(
        feature_view_id=IdHelper.generate_id(),
        version=FrameworkVersion.FWV5.value,
        info=basic_info_pb2.BasicInfo(name=name, description=description, owner=owner, tags=tags),
        prevent_destroy=prevent_destroy,
        entities=[EntityKeyOverride(entity_id=entity._id, join_keys=entity.join_keys) for entity in entities],
        pipeline=pipeline,
        feature_view_type=feature_view_type,
        online_serving_index=online_serving_index if online_serving_index else None,
        online_enabled=online,
        offline_enabled=offline,
        materialized_feature_view_args=MaterializedFeatureViewArgs(
            timestamp_field=timestamp_field,
            feature_start_time=time_utils.datetime_to_proto(feature_start_time),
            batch_schedule=time_utils.timedelta_to_proto(batch_schedule),
            offline_store=offline_store._to_proto(),
            online_store=online_store._to_proto() if online_store else None,
            max_batch_aggregation_interval=time_utils.timedelta_to_proto(max_batch_aggregation_interval),
            monitoring=monitoring._to_proto() if monitoring else None,
            data_source_type=data_source_type,
            incremental_backfills=incremental_backfills,
            batch_trigger=batch_trigger.value,
            output_stream=output_stream._to_proto() if output_stream else None,
            batch_compute=batch_compute_proto,
            stream_compute=stream_compute_proto,
            serving_ttl=time_utils.timedelta_to_proto(ttl),
            stream_processing_mode=stream_processing_mode.value if stream_processing_mode else None,
            aggregations=aggregation_protos,
            aggregation_interval=time_utils.timedelta_to_proto(aggregation_interval),
        ),
    )
